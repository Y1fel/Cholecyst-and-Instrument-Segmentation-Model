# src/training/train_offline_universal.py
"""
é€šç”¨è®­ç»ƒæ¨¡æ¿ - é›†æˆç›‘æ§ã€å¯è§†åŒ–ã€è¯„ä¼°åŠŸèƒ½
åŸºäºtrain_offline_minæ”¹è¿›ï¼Œé€‚ç”¨äºå„ç§æ¨¡å‹çš„è®­ç»ƒ
"""

# $env:PYTHONPATH="F:\Documents\Courses\CIS\Cholecyst-and-Instrument-Segmentation-Model"

import os, argparse, yaml, torch, sys, json
import torch.nn.functional as F
from torch import nn
from torch.utils.data import DataLoader

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# å¯¼å…¥é€šç”¨æ¨¡å—
from src.eval.evaluator import Evaluator
from src.viz.visualizer import Visualizer
from src.common.output_manager import OutputManager
from src.common.train_monitor import TrainMonitor

# ç¤ºä¾‹æ¨¡å‹å¯¼å…¥ - æ ¹æ®å®é™…æ¨¡å‹æ›¿æ¢
from src.dataio.datasets.seg_dataset_min import SegDatasetMin
from src.models.baseline.unet_min import UNetMin

from src.models.model_zoo import build_model
from src.common.constants import compose_mapping

# è’¸é¦ç›¸å…³å¯¼å…¥
try:
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
    from utils.class_distillation import DistillationLoss
    from src.viz.distillation_visualizer import DistillationVisualizer
    DISTILLATION_AVAILABLE = True
except ImportError:
    DISTILLATION_AVAILABLE = False
    print("WARNING: Distillation module not available")

# process arguments
def parse_args():
    """å‚æ•°é…ç½® - å¯æ ¹æ®ä¸åŒæ¨¡å‹éœ€æ±‚è°ƒæ•´"""
    p = argparse.ArgumentParser("Offline Universal Trainer")
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    p.add_argument("--config", "--cfg", type=str, default=None, help="Optional YAML config file path")
    p.add_argument("--data_root", type=str, required=True, help="Dataset root path")
    # p.add_argument("--model_type", type=str, default="universal", help="Model type identifier")
    
    # æ•°æ®å‚æ•°
    p.add_argument("--split", type=str, default="train")
    p.add_argument("--img_size", type=int, default=512)
    p.add_argument("--batch_size", type=int, default=6)
    p.add_argument("--val_ratio", type=float, default=0.2)
    p.add_argument("--num_workers", type=int, default=0)
    
    # è®­ç»ƒå‚æ•°
    p.add_argument("--epochs", type=int, default=5)
    p.add_argument("--lr", type=float, default=3e-4)
    # p.add_argument("--num_classes", type=int, default=2)
    
    # ç›‘æ§å’Œè¾“å‡ºå‚æ•°
    p.add_argument("--monitor_interval", type=int, default=10, help="Progress update interval (batches)")
    p.add_argument("--enable_gpu_monitor", action='store_true', default=True, help="Enable GPU monitoring")
    p.add_argument("--save_viz", action='store_true', help="Save visualizations")
    p.add_argument("--viz_samples", type=int, default=50, help="Number of visualization samples")
    
    # è°ƒè¯•å’Œé«˜çº§é€‰é¡¹
    p.add_argument("--debug", action='store_true', help="Enable debug mode")
    p.add_argument("--save_best_only", action='store_true', default=True, help="Only save best checkpoints")

    # ä»»åŠ¡å®šä¹‰
    p.add_argument("--binary", action="store_true",
                   help="äºŒåˆ†ç±»ï¼ˆèƒ†å›Š+å™¨æ¢°=å‰æ™¯=1ï¼‰ã€‚è‹¥å…³é—­åˆ™æŒ‰å¤šç±»è®­ç»ƒã€‚")
     # çµæ´»åˆ†ç±»é…ç½®
    p.add_argument("--classification_scheme", type=str, default=None,
                   choices=["binary", "3class_org", "3class_balanced", "5class", "detailed", "custom"],
                   help="åˆ†ç±»æ–¹æ¡ˆï¼šbinary(2ç±»), 3class(3ç±»), 3class_balanced(3ç±»å¹³è¡¡ç‰ˆ), 5class(5ç±»), detailed(13ç±»), custom(è‡ªå®šä¹‰)")
    
    p.add_argument("--target_classes", nargs="+", default=None,
                   help="æŒ‡å®šç›®æ ‡ç±»åˆ«åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š--target_classes background instrument target_organ")
    
    p.add_argument("--custom_mapping_file", type=str, default=None,
                   help="è‡ªå®šä¹‰æ˜ å°„JSONæ–‡ä»¶è·¯å¾„")
    
    p.add_argument("--num_classes", type=int, default=10,
                   help="å¤šç±»æ—¶>=2ï¼›--binary ç”Ÿæ•ˆæ—¶å¿½ç•¥æ­¤é¡¹ã€‚watershedæ¨¡å¼å»ºè®®ä½¿ç”¨10ã€‚")
    
    # æ¨¡å‹æ’æ‹”
    p.add_argument("--model", type=str, default="unet_min",
                   choices=["unet_min", "unet_plus_plus", "deeplabv3_plus", "hrnet", "mobile_unet", "adaptive_unet"])
    
    # å…¼å®¹ OutputManager çš„æ¨¡å‹ç±»å‹æ ‡è®°ï¼ˆç”¨äºrunç›®å½•å‘½åï¼‰
    p.add_argument("--model_type", type=str, default=None,
                   help="è‹¥ä¸æŒ‡å®šï¼Œå°†è‡ªåŠ¨ä½¿ç”¨ --model çš„å€¼ã€‚")
    
    # çŸ¥è¯†è’¸é¦å‚æ•°
    p.add_argument("--enable_distillation", action="store_true",
                   help="å¯ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒæ¨¡å¼")
    p.add_argument("--teacher_model", type=str, default="unet_plus_plus",
                   choices=["unet_min", "unet_plus_plus", "deeplabv3_plus", "hrnet"],
                   help="Teacheræ¨¡å‹æ¶æ„")
    p.add_argument("--teacher_checkpoint", type=str, default=None,
                   help="Teacheræ¨¡å‹é¢„è®­ç»ƒæƒé‡è·¯å¾„")
    p.add_argument("--student_model", type=str, default="mobile_unet", 
                   choices=["mobile_unet", "adaptive_unet", "unet_min"],
                   help="Studentæ¨¡å‹æ¶æ„")
    p.add_argument("--distill_temperature", type=float, default=4.0,
                   help="è’¸é¦æ¸©åº¦å‚æ•°")
    p.add_argument("--distill_alpha", type=float, default=0.7,
                   help="è’¸é¦æŸå¤±æƒé‡")
    p.add_argument("--distill_beta", type=float, default=0.3,
                   help="ä»»åŠ¡æŸå¤±æƒé‡")
    p.add_argument("--distill_feature_weight", type=float, default=0.1,
                   help="ç‰¹å¾è’¸é¦æŸå¤±æƒé‡")
    
    # è®­ç»ƒé˜¶æ®µé€‰æ‹©
    p.add_argument("--stage", type=str, default="offline",
                   choices=["offline", "online"], help="æ¨¡å‹è®­ç»ƒé˜¶æ®µ")
    
    # ä¼˜åŒ–å™¨é€‰æ‹©
    p.add_argument("--optimizer", type=str, default="adamw",
                   choices=["adam", "adamw", "sgd", "rmsprop"], help="Optimizer type")
    # SGDåŠ¨é‡
    p.add_argument("--sgd_momentum", type=float, default=0.9, help="Momentum for SGD optimizer")
    
    # å­¦ä¹ ç‡è°ƒåº¦
    p.add_argument("--scheduler", type=str, default="cosine",
                   choices=["none", "step", "cosine", "plateau"], help="Learning rate scheduler type")
    p.add_argument("--weight_decay", type=float, default=1e-4, help="Weight decay for optimizer")

    # æ•°æ®å¢å¼º
    p.add_argument("--augment", action='store_true', default=True, help="Enable data augmentation")
    p.add_argument("--flip_prob", type=float, default=0.5, help="Horizontal flip probability")
    p.add_argument("--rotation_degree", type=int, default=15, help="random rotation range")

    # FOVå¤„ç†
    p.add_argument("--apply_fov_mask", action='store_true', default=False, 
                   help="Apply FOV (Field of View) mask to remove black border regions")

    # éªŒè¯å’Œä¿å­˜
    p.add_argument("--val_interval", type=int, default=1, help="Validation interval (epochs)")
    p.add_argument("--save_interval", type=int, default=5, help="Checkpoint save interval (epochs)")

    # early stopping
    p.add_argument("--early_stopping", action='store_true', help="Enable early stopping")
    p.add_argument("--patience", type=int, default=5, help="Patience for early stopping")

    p.add_argument("--mode", choices=["standard", "kd"], default="standard",
                    help="standard: ä»…GTè®­ç»ƒTeacherï¼›kd: Teacher+StudentçŸ¥è¯†è’¸é¦")
    p.add_argument("--teacher_ckpt", type=str, default="",
                    help="KDæ¨¡å¼ä¸‹Teacherçš„æƒé‡è·¯å¾„")
    p.add_argument("--temperature", type=float, default=4.0)
    p.add_argument("--alpha", type=float, default=0.7,
                    help="æ€»æŸå¤±: alpha*CE + (1-alpha)*KD")
    
    # KD Evidence Package System
    p.add_argument("--generate_evidence_package", action='store_true',
                   help="Generate comprehensive KD evidence package after training")
    p.add_argument("--evidence_samples", type=int, default=500,
                   help="Number of samples for evidence package evaluation")
    p.add_argument("--evidence_experiment_name", type=str, default=None,
                   help="Experiment name for evidence package (auto-detected if None)")
    
    # æ¢å¤è®­ç»ƒå‚æ•°
    p.add_argument("--resume", type=str, default=None,
                   help="Resume training from checkpoint directory (e.g., outputs/model_20250911_230321/checkpoints)")
    p.add_argument("--resume_from_best", action='store_true', default=False,
                   help="Resume from best checkpoint instead of latest epoch checkpoint")
    
    # æ¢å¤æ—¶å¯é€‰çš„å‚æ•°è¦†ç›–ï¼ˆé¢„ç•™æ‰©å±•ï¼‰
    p.add_argument("--resume_lr", type=float, default=None,
                   help="Override learning rate when resuming (optional)")
    p.add_argument("--resume_epochs", type=int, default=None,
                   help="Override total epochs when resuming (optional)")

    return p.parse_args()

# validate arguments
def validate_args(args):
    errors = []    # storage for error messages
    warnings = []  # storage for warning messages

    # Basic parameter validation
    if args.epochs <= 0:
        errors.append("epochs must be positive")
    if args.batch_size <= 0:
        errors.append("batch_size must be positive")
    if args.lr <= 0:
        errors.append("learning rate must be positive")
    if args.num_classes < 1:
        errors.append("num_classes must be >= 1")
    
    # Data parameter validation
    if not (0 < args.val_ratio < 1):
        errors.append("val_ratio must be between 0 and 1")
    if args.img_size < 32:
        warnings.append("img_size < 32 may cause issues")
    
    # Binary classification consistency check
    if args.binary and args.num_classes != 2:
        warnings.append("binary=True but num_classes != 2, using binary mode")
    
    # Monitor parameter validation
    if args.monitor_interval <= 0:
        errors.append("monitor_interval must be positive")
    if args.viz_samples <= 0:
        warnings.append("viz_samples <= 0, visualization disabled")
    
    # Early stopping parameter validation
    if args.early_stopping and args.patience <= 0:
        errors.append("patience must be positive when early_stopping is enabled")

    # Knowledge distillation parameter validation
    if args.enable_distillation and not args.teacher_checkpoint:
        errors.append("KDæ¨¡å¼å¿…é¡»æä¾› --teacher_checkpointï¼Œç¦æ­¢ä½¿ç”¨éšæœºTeacherã€‚")
    
    # Output validation results
    if warnings:
        print("WARNING: Parameter Warnings:")
        for w in warnings:
            print(f"   - {w}")
    
    if errors:
        print("ERROR: Parameter Errors:")
        for e in errors:
            print(f"   - {e}")
        raise ValueError("Invalid parameters detected")
    
    print("OK: Parameter validation passed")
    return True

# use config
def load_config(config_path):
    """åŠ è½½å’ŒéªŒè¯YAMLé…ç½®æ–‡ä»¶"""
    if not config_path:
        return {}
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"OK: Loaded config from: {config_path}")
        return config
    except yaml.YAMLError as e:
        raise ValueError(f"Invalid YAML config: {e}")
    except Exception as e:
        raise ValueError(f"Error loading config: {e}")

# get default parser value
def get_parser_default(param_name):
    # Create a temporary parser to get default values
    temp_parser = argparse.ArgumentParser()

    # Redefine all parameters here (only key ones listed)
    # Add more as needed based on actual usage
    defaults = {
        'epochs': 5,
        'batch_size': 6,
        'lr': 0.0003,
        'num_classes': 10,
        'val_ratio': 0.2,
        'num_workers': 4,
        'img_size': 512,
        'monitor_interval': 5,
        'viz_samples': 50,
        'optimizer': 'adamw',
        'scheduler': 'cosine',
        'weight_decay': 0.0001,
        'early_stopping': False,    # Short-term training default: disabled
        'patience': 5,              # Long-term training recommended value
        'val_interval': 1,          # Validate every epoch
        'save_interval': 1,         # Save checkpoint every x epochs
        'flip_prob': 0.5,
        'rotation_degree': 15,
        'apply_fov_mask': False,
        # æ·»åŠ æ¨¡å‹ç›¸å…³é»˜è®¤å€¼
        'model': 'unet_min',
        # æ·»åŠ è’¸é¦ç›¸å…³é»˜è®¤å€¼
        'enable_distillation': False,
        'teacher_model': 'unet_plus_plus',
        'teacher_checkpoint': None,
        'student_model': 'mobile_unet',
        'distill_temperature': 4.0,
        'distill_alpha': 0.7,
        'distill_beta': 0.3,
        'distill_feature_weight': 0.1,
        # æ·»åŠ åˆ†ç±»ç›¸å…³é»˜è®¤å€¼
        'binary': False,
        'classification_scheme': None,
        'target_classes': None,
        'custom_mapping_file': None,
        # æ·»åŠ å¯è§†åŒ–ç›¸å…³é»˜è®¤å€¼
        'save_viz': False,
        'enable_gpu_monitor': False,
        'augment': False,
        'debug': False,
        'save_best_only': True,
    }
    
    return defaults.get(param_name, None)

# combine config with arguments
def merge_config_with_args(args, config):
    if not config:
        return args

    # Record which parameters are overridden by the config file
    overridden = []
    
    for key, value in config.items():
        if hasattr(args, key):
            # Only use config file value if command line argument is default
            current_value = getattr(args, key)
            parser_default = get_parser_default(key)

            # Only override if current value is default AND config value is different
            if current_value == parser_default and current_value != value:
                setattr(args, key, value)
                overridden.append(f"{key}: {current_value} -> {value}")
    
    if overridden:
        print("CONFIG OVERRIDES:")
        for override in overridden:
            print(f"   - {override}")
    else:
        print("CONFIG: Using default values, no overrides needed")
    
    return args

# compute class weights
def compute_class_weights(dataset, num_classes, ignore_index=255):
    """è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ç±»ä¸å¹³è¡¡"""
    print("Computing class weights from dataset...")
    
    class_counts = np.zeros(num_classes)
    total_pixels = 0
    
    # ç»Ÿè®¡å‰100ä¸ªæ ·æœ¬çš„ç±»åˆ«åˆ†å¸ƒï¼ˆé¿å…è¿‡æ…¢ï¼‰
    sample_size = min(100, len(dataset))
    for i in range(sample_size):
        _, mask = dataset[i]
        mask_np = mask.numpy() if hasattr(mask, 'numpy') else np.array(mask)
        
        for class_id in range(num_classes):
            class_counts[class_id] += np.sum(mask_np == class_id)
        total_pixels += np.sum(mask_np != ignore_index)
    
    # å¤„ç†æ²¡æœ‰æ ·æœ¬çš„ç±»åˆ«
    class_counts = np.maximum(class_counts, 1)  # é¿å…0è®¡æ•°
    
    # è®¡ç®—æƒé‡ï¼ˆå€’æ•° + å¹³æ»‘ï¼‰
    if total_pixels > 0:
        class_weights = total_pixels / (num_classes * class_counts)
        class_weights = class_weights / np.sum(class_weights) * num_classes  # å½’ä¸€åŒ–
    else:
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåƒç´ ï¼Œä½¿ç”¨å‡åŒ€æƒé‡
        class_weights = np.ones(num_classes)
    
    print(f"Class distribution: {class_counts}")
    print(f"Class weights: {class_weights}")
    
    return torch.FloatTensor(class_weights)

# build loss function for segmentation class
class FocalLoss(nn.Module):
    """Focal Loss for addressing class imbalance"""
    def __init__(self, alpha=1, gamma=2, ignore_index=255):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.ignore_index = ignore_index
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, 
                                 ignore_index=self.ignore_index, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()
    
# build optimizer
def create_optimizer(model, args):
    if args.optimizer == "adam":
        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == "adamw":
        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == "sgd":
        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.sgd_momentum, weight_decay=args.weight_decay)
    elif args.optimizer == "rmsprop":
        optimizer = torch.optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    else:
        raise ValueError(f"Unsupported optimizer: {args.optimizer}")
    
    print(f"OK: Created {args.optimizer} optimizer with lr={args.lr}, weight_decay={args.weight_decay}")
    return optimizer

# build scheduler
def create_scheduler(optimizer, args):
    if args.scheduler == "none":
        return None
    elif args.scheduler == "step":
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs//3, gamma=0.5)
    elif args.scheduler == "cosine":
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    elif args.scheduler == "plateau":
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    else:
        raise ValueError(f"Unknown scheduler: {args.scheduler}")
    
    print(f"OK: Created {args.scheduler} scheduler")
    return scheduler

# one epoch training
def train_one_epoch(
    model, loader, criterion, optimizer, device, monitor, epoch_index, args, teacher_model=None
):
    model.train()
    if teacher_model is not None:
        teacher_model.eval()  # Teacherä¿æŒè¯„ä¼°æ¨¡å¼

    running_loss = 0.0
    running_distill_loss = 0.0
    running_task_loss = 0.0
    total = len(loader)

    for step, (images, masks) in enumerate(loader):
        images = images.to(device, non_blocking=True) # [path, 3, H, W]
        masks  = masks.to(device, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)
        

        # Forward pass: images -> logits
        if args.enable_distillation and teacher_model is not None:
            with torch.no_grad():
                teacher_logits = teacher_model(images)
            
            student_logits = model(images)

            # KD é€šé“å¯¹é½æ–­è¨€ï¼šé˜²æ­¢ Teacher/Student é€šé“ä¸ä¸€è‡´å¯¼è‡´è’¸é¦é”™è¯¯
            assert teacher_logits.shape[1] == student_logits.shape[1] == args.num_classes, \
                f"Teacher/Student/num_classesä¸ä¸€è‡´: Teacher={teacher_logits.shape} vs Student={student_logits.shape} vs num_classes={args.num_classes}"

            # ä½¿ç”¨è’¸é¦æŸå¤±
            loss_dict = criterion(student_logits, teacher_logits, masks)
            loss = loss_dict['total_loss']

            running_distill_loss += loss_dict['distill_loss'].item() * images.size(0)
            running_task_loss += loss_dict['task_loss'].item() * images.size(0)

        else:
            logits = model(images)
            if args.binary:
                loss = criterion(logits, masks) # BCEWithLogitsLoss(logits, targets)
            else:
                targets = masks.long()
                loss = criterion(logits, targets)
        
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)

        # monitor progress
        if (step % args.monitor_interval) == 0:
            avg_loss = running_loss / max(1, (step + 1) * args.batch_size)

            if args.enable_distillation and teacher_model is not None:
                avg_distill = running_distill_loss / max(1, (step + 1) * args.batch_size)
                avg_task = running_task_loss / max(1, (step + 1) * args.batch_size)

                monitor.print_progress(
                    epoch_index + 1, args.epochs,
                    step + 1, total,
                    {"total": avg_loss, "distill": avg_distill, "task": avg_task},
                    refresh=True
                )
            else:
                monitor.print_progress(
                    epoch_index + 1, args.epochs,
                    step + 1, total,
                    {"loss": avg_loss},
                    refresh=True
                )

        # Force output every 50 batches to avoid long periods without output
        # if step > 0 and (step % 50) == 0:
        #     avg = running_loss / max(1, (step + 1) * args.batch_size)
        #     print(f"\n[Checkpoint] Epoch {epoch_index + 1}/{args.epochs} Batch {step + 1}/{total} | Loss: {avg:.4f}")
        #     sys.stdout.flush()

    # è¿”å›æŸå¤±ä¿¡æ¯
    dataset_size = len(loader.dataset) if hasattr(loader, 'dataset') else (total * args.batch_size)
    avg_total_loss = running_loss / dataset_size
    
    if args.enable_distillation and teacher_model is not None:
        avg_distill_loss = running_distill_loss / dataset_size
        avg_task_loss = running_task_loss / dataset_size
        return {
            'total_loss': avg_total_loss,
            'distill_loss': avg_distill_loss,
            'task_loss': avg_task_loss
        }
    else:
        return avg_total_loss

# Validation
@torch.inference_mode()
def validate(model, loader, criterion, device, args):
    # åœ¨è’¸é¦æ¨¡å¼ä¸‹ï¼Œåˆ›å»ºæ ‡å‡†éªŒè¯æŸå¤±
    if args.enable_distillation:
        if args.binary:
            val_criterion = nn.BCEWithLogitsLoss()
        else:
            val_criterion = nn.CrossEntropyLoss(ignore_index=255)
    else:
        val_criterion = criterion
    
    if args.binary:
        evaluator = Evaluator(device=device, threshold=0.5)
        return evaluator.evaluate(model, loader, val_criterion)
    else:
        evaluator = Evaluator(device=device)
        return evaluator.evaluate_multiclass(
            model, loader, val_criterion,
            num_classes = args.num_classes,
            ignore_index = 255
        )

# Main function
def generate_kd_evidence_package(args, teacher_model, student_model, val_loader, output_mgr, device):
    """
    Generate comprehensive KD evidence package for analysis
    
    This function creates the unified evidence package that proves KD effectiveness:
    1. Full metrics evaluation (standard + calibration + boundary)
    2. Unified CSV export for comparison tables
    3. KD-specific visualizations (teacher-student analysis, reliability diagrams)
    4. Four-panel KD analysis for presentation
    """
    print("\n" + "="*60)
    print("ğŸ”¬ GENERATING KD EVIDENCE PACKAGE")
    print("="*60)
    
    # Determine experiment type from config or model names
    experiment_name = args.evidence_experiment_name
    if experiment_name is None:
        if hasattr(args, 'teacher_model') and hasattr(args, 'student_model'):
            experiment_name = f"KD_{args.teacher_model}_to_{args.student_model}"
        else:
            experiment_name = "KD_Experiment"
    
    # Initialize components
    evaluator = Evaluator(device=device)
    distill_visualizer = DistillationVisualizer(output_mgr.get_viz_dir(), device)
    visualizer = Visualizer()
    
    print(f"ğŸ“Š Experiment: {experiment_name}")
    print(f"ğŸ“ Output Directory: {output_mgr.get_run_dir()}")
    
    # Step 1: Full metrics evaluation for both models
    print("\nğŸ” Step 1: Comprehensive Metrics Evaluation")
    print("-" * 40)
    
    # Teacher evaluation
    print("   ğŸ“š Evaluating Teacher Model...")
    teacher_metrics = evaluator.evaluate_with_full_metrics(
        teacher_model, val_loader, 
        num_classes=args.num_classes,
        binary_mode=args.binary,
        max_samples=args.evidence_samples
    )
    print(f"      âœ… Teacher - IoU: {teacher_metrics.get('iou', teacher_metrics.get('miou', 0)):.4f}")
    
    # Student evaluation  
    print("   ğŸ“ Evaluating Student Model...")
    student_metrics = evaluator.evaluate_with_full_metrics(
        student_model, val_loader,
        num_classes=args.num_classes, 
        binary_mode=args.binary,
        max_samples=args.evidence_samples
    )
    print(f"      âœ… Student - IoU: {student_metrics.get('iou', student_metrics.get('miou', 0)):.4f}")
    
    # Step 2: Generate unified evidence package
    print("\nğŸ“¦ Step 2: Generating Unified Evidence Package")
    print("-" * 40)
    
    evidence_data = {
        'teacher_metrics': teacher_metrics,
        'student_metrics': student_metrics,
        'experiment_name': experiment_name,
        'training_config': {
            'teacher_model': args.teacher_model if hasattr(args, 'teacher_model') else 'Unknown',
            'student_model': args.student_model if hasattr(args, 'student_model') else 'Unknown', 
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'distill_temperature': getattr(args, 'distill_temperature', args.temperature),
            'distill_alpha': getattr(args, 'distill_alpha', args.alpha)
        }
    }
    
    # Generate evidence package with distillation visualizer (unified format)
    package_paths = distill_visualizer.generate_unified_kd_evidence_package(
        evidence_data, save_prefix=f"{experiment_name}_evidence"
    )
    
    print(f"   âœ… CSV Summary: {os.path.basename(package_paths['csv_path'])}")
    print(f"   âœ… Performance Plot: {os.path.basename(package_paths['performance_plot'])}")
    print(f"   âœ… Reliability Diagrams: {os.path.basename(package_paths['reliability_diagrams'])}")
    
    # Step 3: Four-panel KD analysis
    print("\nğŸ“ˆ Step 3: Four-Panel KD Analysis")
    print("-" * 40)
    
    try:
        four_panel_path = distill_visualizer.create_kd_four_panel_analysis(
            teacher_model, student_model, val_loader,
            temperature=getattr(args, 'distill_temperature', args.temperature),
            max_samples=min(200, args.evidence_samples),  # Limit for memory
            save_name=f"{experiment_name}_four_panel_analysis.png"
        )
        print(f"   âœ… Four-Panel Analysis: {os.path.basename(four_panel_path)}")
    except Exception as e:
        print(f"   âš ï¸  Four-Panel Analysis failed: {str(e)}")
    
    # Step 4: Additional KD-specific analyses
    print("\nğŸ”¬ Step 4: KD-Specific Analysis")
    print("-" * 40)
    
    try:
        # Teacher-Student comparison
        comparison_path = distill_visualizer.visualize_prediction_comparison(
            teacher_model, student_model, val_loader,
            num_samples=min(6, args.evidence_samples // 100),
            save_name=f"{experiment_name}_prediction_comparison.png"
        )
        print(f"   âœ… Prediction Comparison: {os.path.basename(comparison_path)}")
        
        # Knowledge transfer analysis
        kd_stats = distill_visualizer.visualize_knowledge_transfer(
            teacher_model, student_model, val_loader,
            temperature=getattr(args, 'distill_temperature', args.temperature),
            max_samples=min(500, args.evidence_samples),
            save_name=f"{experiment_name}_knowledge_transfer.png"
        )
        print(f"   âœ… Knowledge Transfer Analysis: Generated")
        
    except Exception as e:
        print(f"   âš ï¸  Additional analysis failed: {str(e)}")
    
    print("\n" + "="*60)
    print("âœ… KD EVIDENCE PACKAGE GENERATION COMPLETE")
    print(f"ğŸ“ All files saved in: {output_mgr.get_viz_dir()}")
    print("="*60)
    
    return package_paths

# setup resume training
def setup_resume_training(args):
    """
    è®¾ç½®æ¢å¤è®­ç»ƒ
    Returns:
        (resume_manager, resume_info, start_epoch) æˆ– (None, None, 0)
    """
    if not args.resume:
        return None, None, 0
    
    from src.training.resume_manager import ResumeManager
    
    print("=== RESUME MODE ACTIVATED ===")
    resume_manager = ResumeManager(args.resume)
    resume_info = resume_manager.get_resume_info(args.resume_from_best)
    
    # ä»checkpointä¿¡æ¯ä¸­è·å–èµ·å§‹epoch
    start_epoch = resume_info['checkpoint_info']['epoch']
    
    # å¯é€‰ï¼šä½¿ç”¨åŸå§‹é…ç½®è¦†ç›–å½“å‰argsï¼ˆé¢„ç•™åŠŸèƒ½ï¼‰
    if resume_info['original_config'] and hasattr(args, 'use_original_config'):
        if args.use_original_config:
            original_args = resume_info['original_config']
            for key, value in original_args.items():
                if hasattr(args, key) and getattr(args, key) is None:
                    setattr(args, key, value)
    
    return resume_manager, resume_info, start_epoch

# use resume overrides
def apply_resume_overrides(args):
    """åº”ç”¨æ¢å¤æ—¶çš„å‚æ•°è¦†ç›–"""
    if args.resume_lr is not None:
        print(f"Override learning rate: {args.lr} -> {args.resume_lr}")
        args.lr = args.resume_lr
    
    if args.resume_epochs is not None:
        print(f"Override total epochs: {args.epochs} -> {args.resume_epochs}")
        args.epochs = args.resume_epochs

# load resume states
def load_resume_states(model, optimizer, scheduler, resume_info, device):
    """
    åŠ è½½æ¢å¤çŠ¶æ€åˆ°æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨
    """
    checkpoint_info = resume_info['checkpoint_info']
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    if checkpoint_info['model_state_dict']:
        model.load_state_dict(checkpoint_info['model_state_dict'])
        print(f"âœ“ Model state loaded from epoch {checkpoint_info['epoch']}")
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if checkpoint_info['optimizer_state_dict'] and optimizer:
        optimizer.load_state_dict(checkpoint_info['optimizer_state_dict'])
        print(f"âœ“ Optimizer state loaded")
    
    # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
    if checkpoint_info['scheduler_state_dict'] and scheduler:
        scheduler.load_state_dict(checkpoint_info['scheduler_state_dict'])
        print(f"âœ“ Scheduler state loaded")

# main function
def main():
    args = parse_args()

    # setup resume training
    resume_manager, resume_info, start_epoch = setup_resume_training(args)

    # apply resume overrides
    if resume_manager:
        apply_resume_overrides(args)

    # load config and validate args
    config = load_config(args.config)
    args = merge_config_with_args(args, config)
    validate_args(args)

    # Binary/Multiclass å¼ºä¸€è‡´æ€§ä¿æŠ¤ï¼šè‡ªåŠ¨ä¿®æ­£ num_classes
    if args.binary:
        if args.num_classes != 2:
            print(f"[WARN] binary=True ä½† num_classes={args.num_classes} != 2ï¼Œè‡ªåŠ¨å°† num_classes ç½®ä¸º 2")
            args.num_classes = 2

    # Print save strategy description
    print(f"SAVE STRATEGY:")
    print(f"   - Best model: Save when validation improves")
    print(f"   - Checkpoints: Every {args.save_interval} epochs")
    print(f"   - Validation: Every {args.val_interval} epoch(s)")
    if args.early_stopping:
        print(f"   - Early stopping: Enabled (patience={args.patience})")
    else:
        print(f"   - Early stopping: Disabled")

    # device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # train monitor
    monitor = TrainMonitor(enable_gpu_monitor=args.enable_gpu_monitor)
    monitor.start_timing()

    # output manager
    if args.enable_distillation:
        model_tag = f"distill_{args.teacher_model}_to_{args.student_model}"
    else:
        model_tag = args.model if args.model_type is None else args.model_type
    
    # æ¢å¤è®­ç»ƒæ—¶ä½¿ç”¨åŸæ¥çš„è¾“å‡ºç›®å½•
    if resume_manager:
        # ä½¿ç”¨åŸæ¥çš„runç›®å½•
        original_run_dir = resume_info['run_dir']
        output_mgr = OutputManager(model_type=model_tag)
        # è¦†ç›–run_dirä¸ºåŸæ¥çš„ç›®å½•
        output_mgr.run_dir = original_run_dir
        output_mgr._setup_directories()  # ç¡®ä¿ç›®å½•å­˜åœ¨
        print(f"=== RESUME: Using original output directory: {original_run_dir} ===")
    else:
        # æ­£å¸¸è®­ç»ƒï¼šåˆ›å»ºæ–°çš„è¾“å‡ºç›®å½•
        output_mgr = OutputManager(model_type=model_tag)
        output_mgr.save_config(vars(args))  # æ­£å¸¸è®­ç»ƒæ—¶ä¿å­˜é…ç½®

    # load custom mapping
    custom_mapping = None
    if args.custom_mapping_file:
        with open(args.custom_mapping_file, 'r') as f:
            custom_mapping = json.load(f)
    
    # ä½¿ç”¨compose_mappingç”Ÿæˆæœ€ç»ˆçš„watershed -> target classæ˜ å°„
    class_id_map = compose_mapping(
        classification_scheme=args.classification_scheme,
        custom_mapping=custom_mapping,
        target_classes=args.target_classes
    )

    print(f"[MAPPING] Generated class_id_map with {len(class_id_map)} entries")
    print(f"[MAPPING] Target classes: {list(set(class_id_map.values()))}")

    # Dataloader  
    is_multiclass = (not args.binary) and (args.num_classes >= 2)

    # Dataset configuration
    dataset_config = {
        # "classification_scheme": args.classification_scheme,
        # "custom_mapping": custom_mapping,
        # "target_classes": args.target_classes,
        "class_id_map": class_id_map,  # ç›´æ¥ä¼ å…¥æœ€ç»ˆæ˜ å°„
        "return_multiclass": is_multiclass, # ä¿æŒå…¼å®¹æ€§
        "apply_fov_mask": args.apply_fov_mask  # FOV maskå¤„ç†
    }

    full_dataset = SegDatasetMin(
        args.data_root, dtype=args.split, img_size=args.img_size,
        **dataset_config
    )

    # ç”¨æ•°æ®é›†çœŸå®ç±»æ•°å›å†™ args.num_classes
    # é˜²æ­¢CE/BCEé€šé“æ•°ä¸å®é™…æ˜ å°„ä¸ä¸€è‡´å¯¼è‡´çš„å…¨255/å…¨èƒŒæ™¯/é¢œè‰²é”™ä¹±é—®é¢˜
    if not args.binary:
        original_num_classes = args.num_classes
        args.num_classes = getattr(full_dataset, "num_classes", args.num_classes)
        if args.num_classes != original_num_classes:
            print(f"[DATASET] Updated num_classes: {original_num_classes} -> {args.num_classes} (from dataset)")
        else:
            print(f"[DATASET] Confirmed num_classes: {args.num_classes} (matches dataset)")

    # æ ‡ç­¾å¥åº·æ£€æŸ¥ï¼šå¿«é€Ÿæ£€æµ‹æ˜¯å¦æ‰€æœ‰æ ‡ç­¾éƒ½è¢«æ˜ å°„ä¸º255ï¼ˆignoreï¼‰
    print("[HEALTH CHECK] Checking label distribution in first 20 samples...")
    from collections import Counter
    valid_counter = Counter()
    sample_size = min(200, len(full_dataset))
    
    # check first N samples' labels
    for i in range(sample_size):
        try:
            _, mask = full_dataset[i]  # å‡è®¾ __getitem__ è¿”å› image, mask
            mask_tensor = mask if torch.is_tensor(mask) else torch.tensor(mask)
            unique_values = torch.unique(mask_tensor)
            valid_values = unique_values[unique_values != 255]  # æ’é™¤ignoreæ ‡ç­¾
            valid_counter.update(valid_values.cpu().tolist())
            
            lab = mask_tensor.numpy()
            valid = lab[lab != 255]
            # if valid.size == 0:
            #     print(f"[HEALTH CHECK] sample#{i}: only ignore")
            # else:
            #     u, c = np.unique(valid, return_counts=True)
            #     print(f"[HEALTH CHECK] sample#{i}: {dict(zip(u.tolist(), c.tolist()))}")
        except Exception as e:
            print(f"[HEALTH CHECK] Warning: Failed to check sample {i}: {e}")
            continue
    
    if len(valid_counter) == 0:
        raise ValueError(
            "[FAILED] æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾éƒ½åªæœ‰ 255ï¼ˆignoreï¼‰ï¼Œè¯·æ£€æŸ¥ class_id_map / æ˜ å°„æµç¨‹ã€‚\n"
            "å¯èƒ½åŸå› ï¼š\n"
            "  1. class_id_map æ˜ å°„é”™è¯¯ï¼Œæ‰€æœ‰åŸå§‹æ ‡ç­¾éƒ½è¢«æ˜ å°„ä¸º255\n"
            "  2. æ•°æ®é›†è·¯å¾„é”™è¯¯æˆ–æ ‡ç­¾æ–‡ä»¶æŸå\n"
            "  3. æ˜ å°„å‡½æ•°é€»è¾‘é”™è¯¯ï¼Œæœªæ­£ç¡®å¤„ç†ç›®æ ‡ç±»åˆ«"
        )
    else:
        valid_classes = sorted(valid_counter.keys())
        total_valid_pixels = sum(valid_counter.values())
        print(f"[PASS] [HEALTH CHECK] å‘ç°æœ‰æ•ˆæ ‡ç­¾: {valid_classes}")
        print(f"[PASS] [HEALTH CHECK] æ ‡ç­¾åˆ†å¸ƒ: {dict(valid_counter)} (å…± {total_valid_pixels:,} ä¸ªæœ‰æ•ˆåƒç´ )")
        
        # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æœ‰æ•ˆç±»åˆ«æ•°ä¸é¢„æœŸåŒ¹é…
        if not args.binary and len(valid_classes) > args.num_classes:
            print(f"[WARN] [HEALTH CHECK] Warning: å‘ç° {len(valid_classes)} ä¸ªæœ‰æ•ˆç±»åˆ«ï¼Œä½† num_classes={args.num_classes}")

    # split ratio
    val_ratio  = args.val_ratio
    val_size   = int(len(full_dataset) * val_ratio)
    train_size = len(full_dataset) - val_size

    # create datasets with random split and seed
    seed = 42
    train_ds, val_ds = torch.utils.data.random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)
    val_loader   = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)
    print(f"Dataset: Train={len(train_ds)}, Val={len(val_ds)}")

    # Model å’Œ è’¸é¦è®¾ç½®
    if args.enable_distillation:
        if not DISTILLATION_AVAILABLE:
            raise ImportError("Distillation requested but class_distillation module not available")

        print(f"=== Knowledge Distillation Mode Enabled ===")
        print(f"Teacher: {args.teacher_model}, Student: {args.student_model}")
        print(f"Temperature: {args.distill_temperature}, Alpha: {args.distill_alpha}, Beta: {args.distill_beta}")
        
        # Build Teacher and Student models  
        teacher_model = build_model(args.teacher_model, num_classes=args.num_classes, in_ch=3, stage="offline").to(device)
        student_model = build_model(args.student_model, num_classes=args.num_classes, in_ch=3, stage="online").to(device)

        # Load pretrained Teacher weights if provided
        if args.teacher_checkpoint:
            print(f"Loading Teacher model weights from: {args.teacher_checkpoint}")
            try:
                if not os.path.exists(args.teacher_checkpoint):
                    raise FileNotFoundError(f"Teacher checkpoint not found: {args.teacher_checkpoint}")
                
                teacher_checkpoint = torch.load(args.teacher_checkpoint, map_location=device, weights_only=False)
                
                # Handle different checkpoint formats
                if 'model_state_dict' in teacher_checkpoint:
                    state_dict = teacher_checkpoint['model_state_dict']
                    print(f"[TEACHER] Loading from 'model_state_dict' format")
                elif 'state_dict' in teacher_checkpoint:
                    state_dict = teacher_checkpoint['state_dict']
                    print(f"[TEACHER] Loading from 'state_dict' format")
                else:
                    state_dict = teacher_checkpoint
                    print(f"[TEACHER] Loading from direct state_dict format")
                
                # æ£€æŸ¥æ¨¡å‹å…¼å®¹æ€§
                teacher_state_keys = set(teacher_model.state_dict().keys())
                checkpoint_keys = set(state_dict.keys())
                
                missing_keys = teacher_state_keys - checkpoint_keys
                unexpected_keys = checkpoint_keys - teacher_state_keys
                
                if missing_keys:
                    print(f"[TEACHER] Missing keys: {len(missing_keys)} (will be randomly initialized)")
                    if len(missing_keys) <= 5:
                        print(f"[TEACHER] Missing: {list(missing_keys)}")
                
                if unexpected_keys:
                    print(f"[TEACHER] Unexpected keys: {len(unexpected_keys)} (will be ignored)")
                    if len(unexpected_keys) <= 5:
                        print(f"[TEACHER] Unexpected: {list(unexpected_keys)}")
                
                # åŠ è½½æƒé‡ï¼ˆå¿½ç•¥ä¸åŒ¹é…çš„é”®ï¼‰
                teacher_model.load_state_dict(state_dict, strict=False)
                
                # éªŒè¯åŠ è½½ç»“æœ
                loaded_params = sum(p.numel() for p in teacher_model.parameters())
                print(f"[PASS] Teacher model weights loaded successfully")
                print(f"[TEACHER] Total parameters: {loaded_params:,}")
                
                # å†»ç»“Teacheræ¨¡å‹å‚æ•°
                for param in teacher_model.parameters():
                    param.requires_grad = False
                teacher_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                print(f"[TEACHER] Model frozen and set to eval mode")
                
                # éªŒè¯Teacheræ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸å‰å‘æ¨ç†
                with torch.no_grad():
                    test_input = torch.randn(1, 3, args.img_size, args.img_size).to(device)
                    test_output = teacher_model(test_input)
                    output_stats = {
                        'shape': test_output.shape,
                        'min': test_output.min().item(),
                        'max': test_output.max().item(),
                        'mean': test_output.mean().item(),
                        'std': test_output.std().item()
                    }
                    print(f"[TEACHER] Test forward pass successful:")
                    print(f"[TEACHER]   Input: {test_input.shape} -> Output: {test_output.shape}")
                    print(f"[TEACHER]   Output stats: min={output_stats['min']:.4f}, max={output_stats['max']:.4f}, mean={output_stats['mean']:.4f}, std={output_stats['std']:.4f}")
                    
                    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦æœ‰æ„ä¹‰ï¼ˆä¸æ˜¯å…¨é›¶æˆ–å…¨NaNï¼‰
                    if torch.all(test_output == 0):
                        print(f"[WARN] [TEACHER] WARNING: Model outputs all zeros!")
                    elif torch.isnan(test_output).any():
                        print(f"[WARN] [TEACHER] WARNING: Model outputs contain NaN!")
                    else:
                        print(f"[PASS] [TEACHER] Model output appears normal")
                    
            except Exception as e:
                print(f"[FAILED] Error loading Teacher weights: {e}")
                print("[WARN] Continuing with randomly initialized Teacher model")
                print("[WARN] This will result in poor distillation performance")
                
                # å³ä½¿åŠ è½½å¤±è´¥ï¼Œä¹Ÿè¦å†»ç»“Teacher
                for param in teacher_model.parameters():
                    param.requires_grad = False
                teacher_model.eval()
        else:
            print("[WARN] No Teacher checkpoint provided - using randomly initialized Teacher model")
            print("[WARN] This will result in poor distillation performance")
            # å†»ç»“éšæœºåˆå§‹åŒ–çš„Teacher
            for param in teacher_model.parameters():
                param.requires_grad = False
            teacher_model.eval()

        # Use distillation loss
        criterion = DistillationLoss(
            num_classes=args.num_classes,  # ç›´æ¥ä½¿ç”¨args.num_classesï¼Œä¸å†æ ¹æ®binaryåˆ¤æ–­
            temperature=args.distill_temperature,
            alpha=args.distill_alpha,
            beta=args.distill_beta,
            feature_weight=args.distill_feature_weight,
            ignore_index=255  # æ·»åŠ ignore_indexä»¥å¿½ç•¥æ— æ•ˆåƒç´ 
        )

        # Train the Student model primarily
        model = student_model
    else:
        # åŸæœ‰çš„å•æ¨¡å‹è®­ç»ƒæ¨¡å¼
        print(f"=== Standard Training Mode ===")
        print(f"Model: {args.model}")    
        # Model
        if args.binary:
            # äºŒåˆ†ç±»ï¼šæ¨¡å‹è¾“å‡º1ä¸ªé€šé“ï¼Œç”¨äºBCEWithLogitsLoss
            model = build_model(args.model, num_classes=1, in_ch=3, stage=args.stage).to(device)
            criterion = nn.BCEWithLogitsLoss()  # äºŒåˆ†ç±»ç”¨BCE
        else:
            # å¤šåˆ†ç±»ï¼šæ¨¡å‹è¾“å‡ºnum_classesä¸ªé€šé“ï¼Œç”¨äºCrossEntropyLoss  
            model = build_model(args.model, num_classes=args.num_classes, in_ch=3, stage=args.stage).to(device)
            
            # è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            print("Computing class weights to handle class imbalance...")
            class_weights = compute_class_weights(full_dataset, args.num_classes, ignore_index=255)
            class_weights = class_weights.to(device)
            print(f"Class weights: {class_weights}")
            
            # ä½¿ç”¨åŠ æƒCrossEntropyLossæ¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆæ¨èæ–¹æ¡ˆï¼‰
            criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=255)
            # criterion = FocalLoss(alpha=1.0, gamma=2.0, ignore_index=255)  # å¤‡é€‰ï¼šFocal Loss
        teacher_model = None  # æ ‡å‡†æ¨¡å¼ä¸‹æ²¡æœ‰Teacheræ¨¡å‹

    # Optimizer and Scheduler
    optimizer = create_optimizer(model, args)
    scheduler = create_scheduler(optimizer, args)
    
    best_val_loss = float("inf")
    patience_counter = 0  # Initialize early stopping counter

    print("=" * 80) # Training start
    if args.enable_distillation:
        print(f" Knowledge Distillation Training: {args.teacher_model} â†’ {args.student_model}")
        print(f"     Teacher: {args.teacher_model} (frozen, providing soft targets)")
        print(f"     Student: {args.student_model} (learning, will be deployed)")
        
        # åˆå§‹åŒ–è’¸é¦æŒ‡æ ‡æ”¶é›†
        distillation_metrics = {
            'total_loss': [],
            'task_loss': [],
            'distill_loss': [],
            'val_loss': [],
            'miou': [],
            'mdice': [],
            'macc': []
        }
    else:
        print(f" Standard Training: {args.model}")
    
    # æ”¹è¿›epochæ˜¾ç¤ºé€»è¾‘
    if resume_manager:
        remaining_epochs = max(0, args.epochs - start_epoch)
        print(f"Training: Resume from epoch {start_epoch} â†’ Continue to epoch {args.epochs-1} (æ€»å…±{remaining_epochs}ä¸ªæ–°epoch)")
        if remaining_epochs == 0:
            print(f"âš ï¸  WARNING: å·²åˆ°è¾¾ç›®æ ‡epoch ({args.epochs})ï¼Œæ— éœ€ç»§ç»­è®­ç»ƒï¼")
            print(f"   å»ºè®®ä½¿ç”¨ --epochs {start_epoch + 5} æˆ–æ›´é«˜çš„å€¼æ¥ç»§ç»­è®­ç»ƒ")
    else:
        print(f"Training: Start from epoch 0 â†’ Train to epoch {args.epochs-1} (æ€»å…±{args.epochs}ä¸ªepoch)")

    # if resume, load states
    if resume_manager:
        load_resume_states(model, optimizer, scheduler, resume_info, device)
        print(f"=== RESUMING FROM EPOCH {start_epoch} ===")

    # Training loop - æ£€æŸ¥æ˜¯å¦æœ‰epochéœ€è¦è®­ç»ƒ
    if start_epoch >= args.epochs:
        print(f"ğŸ›‘ è®­ç»ƒå·²å®Œæˆï¼å½“å‰epoch ({start_epoch}) >= ç›®æ ‡epoch ({args.epochs})")
        print(f"   å¦‚éœ€ç»§ç»­è®­ç»ƒï¼Œè¯·ä½¿ç”¨ --epochs {start_epoch + 10} ç­‰æ›´é«˜çš„å€¼")
        return
    
    for epoch in range(start_epoch, args.epochs):
        # Train for one epoch
        if args.enable_distillation:
            train_results = train_one_epoch(model, train_loader, criterion, optimizer, device, monitor, epoch, args, teacher_model)
            avg_train = train_results['total_loss']
            
            # æ”¶é›†è’¸é¦æŒ‡æ ‡
            distillation_metrics['total_loss'].append(train_results['total_loss'])
            distillation_metrics['task_loss'].append(train_results['task_loss'])
            distillation_metrics['distill_loss'].append(train_results['distill_loss'])
        else:
            avg_train = train_one_epoch(model, train_loader, criterion, optimizer, device, monitor, epoch, args)

        print(f"Epoch [{epoch + 1}/{args.epochs}], Train Loss: {avg_train:.4f}")

        val_metrics = validate(model, val_loader, criterion, device, args) # Validate model
        
        # åœ¨è’¸é¦æ¨¡å¼ä¸‹æ”¶é›†éªŒè¯æŒ‡æ ‡
        if args.enable_distillation:
            distillation_metrics['val_loss'].append(val_metrics['val_loss'])
            if 'miou' in val_metrics:
                distillation_metrics['miou'].append(val_metrics['miou'])
            if 'mdice' in val_metrics:
                distillation_metrics['mdice'].append(val_metrics['mdice'])
            if 'macc' in val_metrics:
                distillation_metrics['macc'].append(val_metrics['macc'])

        if args.binary: # Binary classification
            print(f"[Epoch {epoch+1}] "
                f"Val loss: {val_metrics['val_loss']:.4f} | "
                f"IoU: {val_metrics['iou']:.4f} | Dice: {val_metrics['dice']:.4f} | "
                f"Acc: {val_metrics['accuracy']:.4f} | Prec: {val_metrics['precision']:.4f} | Rec: {val_metrics['recall']:.4f}")
        else: # Multi-class classification
            print(f"[Epoch {epoch+1}] "
                f"Val loss: {val_metrics['val_loss']:.4f} | "
                f"mIoU: {val_metrics['miou']:.4f} | mDice: {val_metrics['mdice']:.4f} | mAcc: {val_metrics['macc']:.4f}")

        # Combine metrics for logging
        combined_metrics = {"train_loss": avg_train}
        combined_metrics.update(val_metrics)
        output_mgr.save_metrics_csv(combined_metrics, epoch + 1)

        if scheduler is not None: # Learning rate scheduler
            if args.scheduler == "plateau": # Plateau scheduler
                scheduler.step(val_metrics['val_loss'])
            else:
                scheduler.step()

        # Early stopping logic (before saving)
        current_val_loss = val_metrics['val_loss']
        is_improvement = current_val_loss < best_val_loss
        
        if args.early_stopping:
            if is_improvement:
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= args.patience:
                    print(f"Early stopping triggered after {args.patience} epochs without improvement")
                    break

        # save checkpoints using the unified method
        if args.enable_distillation:
            # çŸ¥è¯†è’¸é¦æ¨¡å¼ï¼šåªä¿å­˜Studentæ¨¡å‹ï¼ˆTeacheræ˜¯å†»ç»“çš„ï¼Œæ— éœ€ä¿å­˜ï¼‰
            # ä¿å­˜Studentæ¨¡å‹ï¼ˆä¸»è¦è®­ç»ƒæ¨¡å‹ï¼‰
            saved_path, best_val_loss = output_mgr.save_checkpoint_if_needed(
                model=student_model,
                epoch=epoch + 1,
                metrics=val_metrics,
                current_best_metric=best_val_loss,
                metric_name='val_loss',
                minimize=True,
                save_interval=args.save_interval,
                model_suffix="student"
            )
            
            # Teacheræ¨¡å‹åœ¨ç¬¬ä¸€ä¸ªepochæ—¶ä¿å­˜ä¸€æ¬¡ä½œä¸ºå‚è€ƒï¼Œåç»­ä¸å†ä¿å­˜
            if epoch == 0:
                teacher_reference_path = output_mgr.save_model(
                    teacher_model, 
                    epoch + 1, 
                    val_metrics, 
                    is_best=False, 
                    model_suffix="teacher_reference"
                )
                print(f"SAVED: Teacher reference model (frozen): {os.path.basename(teacher_reference_path)}")
        else:
            # æ ‡å‡†è®­ç»ƒæ¨¡å¼ï¼šåªä¿å­˜å•ä¸ªæ¨¡å‹
            saved_path, best_val_loss = output_mgr.save_checkpoint_if_needed(
                model=model,
                epoch=epoch + 1,
                metrics=val_metrics,
                current_best_metric=best_val_loss,
                metric_name='val_loss',
                minimize=True,
                save_interval=args.save_interval
            )

        # Add epoch summary (migrated from train_offline_min)
        train_metrics = {"loss": avg_train}
        monitor.print_epoch_summary(epoch + 1, train_metrics, val_metrics)


    # Visualization after training (outside training loop)
    if args.save_viz and 'val_metrics' in locals():
        print("\n-- Loading Best Model for Visualization --")
        
        # Find the best model file
        checkpoints_dir = output_mgr.get_checkpoints_dir()
        
        if args.enable_distillation:
            # è’¸é¦æ¨¡å¼ï¼šåŠ è½½Studentæœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(checkpoints_dir, f"{model_tag}_student_best.pth")
        else:
            best_model_path = os.path.join(checkpoints_dir, f"{model_tag}_best.pth")
        
        if os.path.exists(best_model_path): # Load best model
            checkpoint = torch.load(best_model_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ“ Loaded best model from {best_model_path}")
        else:
            print("WARNING: Best model not found, using current model for visualization")
        
        print("-- Saving Visualizations --")
        visualizer = Visualizer()
        viz_dir = output_mgr.get_vis_dir() # Get visualization directory

        # Save visualization results
        visualizer.save_comparison_predictions(
            model, val_loader, viz_dir, max_samples=args.viz_samples, device=device
        )
        visualizer.save_basic_predictions(
            model, val_loader, viz_dir, max_samples=args.viz_samples, device=device
        )
        
        # è’¸é¦æ¨¡å¼ç‰¹æœ‰çš„å¯è§†åŒ–
        if args.enable_distillation and DISTILLATION_AVAILABLE:
            print("-- Generating Distillation Analysis --")
            distill_visualizer = DistillationVisualizer(viz_dir, device)
            
            # Teacher-Studenté¢„æµ‹å¯¹æ¯”
            distill_visualizer.visualize_prediction_comparison(
                teacher_model, student_model, val_loader, 
                num_samples=6, save_name="teacher_student_comparison.png"
            )
            
            # çŸ¥è¯†ä¼ é€’åˆ†æ
            distillation_stats = distill_visualizer.visualize_knowledge_transfer(
                teacher_model, student_model, val_loader,
                temperature=args.distill_temperature,
                max_samples=1000,  # Limit samples to prevent memory overflow
                save_name="knowledge_transfer_analysis.png"
            )
            
            # ç”Ÿæˆè’¸é¦æ€»ç»“æŠ¥å‘Š
            distill_visualizer.create_distillation_summary_report(
                distillation_metrics, distillation_stats,
                args.teacher_model, args.student_model,
                save_name="distillation_summary_report.png"
            )
            
            # ä¿å­˜è’¸é¦æŒ‡æ ‡è¡¨æ ¼
            distill_visualizer.save_distillation_metrics_table(
                distillation_metrics, distillation_stats,
                save_name="distillation_metrics.csv"
            )
            
            print(f"[PASS] è’¸é¦åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {viz_dir}/distillation_analysis/")

    # Final model saving (using the last epoch's metrics)
    if 'val_metrics' in locals():
        if args.enable_distillation:
            # è’¸é¦æ¨¡å¼ï¼šåªä¿å­˜æœ€ç»ˆçš„Studentæ¨¡å‹ï¼ˆTeacherå·²åœ¨ç¬¬ä¸€è½®ä¿å­˜è¿‡å‚è€ƒç‰ˆæœ¬ï¼‰
            student_final_path = output_mgr.save_model(student_model, args.epochs, val_metrics, is_best=False, model_suffix="student_final")
            print(f"SAVED: Final student model saved: {os.path.basename(student_final_path)}")
            print(f"NOTE: Teacher model was saved as reference in epoch 1 (frozen model, no updates)")
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šä¿å­˜å•ä¸ªæ¨¡å‹
            final_path = output_mgr.save_model(model, args.epochs, val_metrics, is_best=False)
            print(f"SAVED: Final model saved: {os.path.basename(final_path)}")

    # Print summary
    summary = output_mgr.get_run_summary()
    print(f"\n--> Train Completed <--")
    print(f"Results saved to: {summary['run_dir']}")
    
    # KD Evidence Package Generation (if enabled)
    if args.generate_evidence_package and args.enable_distillation and DISTILLATION_AVAILABLE:
        print("\nğŸ”¬ Generating KD Evidence Package...")
        try:
            evidence_paths = generate_kd_evidence_package(
                args, teacher_model, student_model, val_loader, output_mgr, device
            )
            print(f"âœ… Evidence package generated successfully!")
            print(f"ğŸ“Š Key outputs:")
            print(f"   - Metrics CSV: {os.path.basename(evidence_paths['csv_path'])}")
            print(f"   - Performance Analysis: {os.path.basename(evidence_paths['performance_plot'])}")
            print(f"   - Reliability Diagrams: {os.path.basename(evidence_paths['reliability_diagrams'])}")
        except Exception as e:
            print(f"âš ï¸  Evidence package generation failed: {str(e)}")
            print("   Training completed successfully, but evidence package could not be generated.")
    elif args.generate_evidence_package and not args.enable_distillation:
        print("âš ï¸  Evidence package requested but distillation not enabled. Skipping evidence generation.")
    elif args.generate_evidence_package and not DISTILLATION_AVAILABLE:
        print("âš ï¸  Evidence package requested but distillation modules not available. Skipping evidence generation.")
    
    # ä¿å­˜æŒ‡æ ‡æ›²çº¿å›¾
    if 'monitor' in locals():
        viz_visualizer = Visualizer()
        metrics_history = monitor.get_metrics_history()
        if metrics_history:
            curves_path = output_mgr.get_viz_path("training_curves.png")
            viz_visualizer.plot_metrics_curves(metrics_history, curves_path)
            print(f"Training curves saved to: {curves_path}")
    
    if 'val_metrics' in locals():
        if args.binary:
            print(f"Final Metrics - Loss: {val_metrics['val_loss']:.4f}, IoU: {val_metrics['iou']:.4f}, Dice: {val_metrics['dice']:.4f}, Acc: {val_metrics['accuracy']:.4f}")
        else:
            print(f"Final Metrics - Loss: {val_metrics['val_loss']:.4f}, mIoU: {val_metrics['miou']:.4f}, mDice: {val_metrics['mdice']:.4f}")
    else:
        print("Training completed but no validation metrics available")

if __name__ == "__main__":
    main()

# 
import os, csv, json, torch
from datetime import datetime
from typing import Dict, Any, Optional

class OutputManager:
    def __init__(self, model_type: str = "baseline", output_dir: str = "./outputs", run_dir: str = None):
        self.model_type = model_type
        self.output_dir = output_dir
        
        if run_dir is not None:
            # ä½¿ç”¨æŒ‡å®šçš„runç›®å½•ï¼ˆæ¢å¤è®­ç»ƒæ—¶ï¼‰
            self.run_dir = run_dir
            self.timestamp = os.path.basename(run_dir).split('_')[-1] if '_' in os.path.basename(run_dir) else "resumed"
        else:
            # åˆ›å»ºæ–°çš„runç›®å½•ï¼ˆæ­£å¸¸è®­ç»ƒæ—¶ï¼‰
            self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.run_dir = os.path.join(self.output_dir, f"{self.model_type}_{self.timestamp}")

        # build basic directory structure
        self._setup_directories()

    # Setup directories for the current run
    def _setup_directories(self):
        # construct basic dir
        dirs = [
            self.run_dir,
            self.get_vis_dir(),
            self.get_checkpoints_dir(),
        ]

        for dir_path in dirs: # loop and make
            os.makedirs(dir_path, exist_ok=True)

    # Get run directory path
    def get_run_dir(self) -> str:
        return self.run_dir

    # Get path for visualizations
    def get_vis_dir(self) -> str:
        return os.path.join(self.run_dir, "visualizations")

    # Get path for specific visualization file
    def get_viz_path(self, filename: str) -> str:
        return os.path.join(self.get_vis_dir(), filename)

    # Get path for checkpoints
    def get_checkpoints_dir(self) -> str:
        return os.path.join(self.run_dir, "checkpoints")

    # Get path for metrics CSV
    def get_metrics_csv_path(self) -> str:
        return os.path.join(self.run_dir, "metrics.csv")

    # Save config to JSON
    def save_config(self, config: Dict):
        config_path = os.path.join(self.run_dir, "config.json")
        config_with_meta = {
            "timestamp": self.timestamp,
            "model_type": self.model_type,
            "config": config
        }

        with open(config_path, "w") as f:
            json.dump(config_with_meta, f, indent=2)

        print(f"Config saved to: {config_path}")

    # Save metrics to CSV
    def save_metrics_csv(self, metrics: Dict, epoch: int):
        # Append metrics to CSV
        csv_path = self.get_metrics_csv_path()

        # data row
        row_data = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "epoch": epoch,
        }
        row_data.update(metrics)  #ä¿®å¤ï¼šæ·»åŠ å®é™…çš„æŒ‡æ ‡æ•°æ®

        # check header
        write_header = not os.path.exists(csv_path)

        with open(csv_path, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=row_data.keys())
            if write_header: # if file is new, write header first
                writer.writeheader()
            writer.writerow(row_data) # write the data row

    # obtain run summary
    def get_run_summary(self) -> Dict: # get summary of current run
        return {
            "run_dir": self.run_dir,
            "timestamp": self.timestamp,
            "model_type": self.model_type,
            "file": {
                "metrics_csv": self.get_metrics_csv_path(),
                "vis_dir": self.get_vis_dir(),
                "checkpoints_dir": self.get_checkpoints_dir(),
            }
        }

    # save model
    def save_model(self, model, epoch: int, metrics:Dict, is_best: bool = False, model_suffix: str = ""):
        checkpoint = { # Create a checkpoint dictionary
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'metrics': metrics,
            'timestamp': self.timestamp,
            'is_best': is_best
        }

        # æ„å»ºæ–‡ä»¶åï¼Œæ”¯æŒmodel_suffix
        model_name = f"{self.model_type}_{model_suffix}" if model_suffix else self.model_type

        # save every checkpoint
        epoch_path = os.path.join(
            self.get_checkpoints_dir(),
            f"{model_name}_epoch_{epoch:03d}.pth"
        )
        torch.save(checkpoint, epoch_path)
        suffix_info = f" ({model_suffix})" if model_suffix else ""
        print(f"âœ“ Saved checkpoint{suffix_info}: {os.path.basename(epoch_path)}")

        # if best, save a copy as best
        if is_best:
            best_checkpoint = checkpoint.copy()
            best_checkpoint['best_epoch'] = epoch
            
            best_path = os.path.join(
                self.get_checkpoints_dir(),
                f"{model_name}_best.pth"
            )
            torch.save(best_checkpoint, best_path)
            print(f"BEST: Saved best model{suffix_info}: {os.path.basename(best_path)}")
            
            # update best model record
            self._update_best_model_record(epoch, metrics, best_path)
            
            return best_path
        
        return epoch_path

    # save checkpoint by interval
    def save_checkpoint_if_needed(self, model, epoch: int, metrics: Dict, 
                                current_best_metric: float, metric_name: str = 'val_loss',
                                minimize: bool = True, save_interval: int = 5, 
                                model_suffix: str = "", save_best: bool = True):
        """
        æ™ºèƒ½ä¿å­˜checkpointï¼Œè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        Args:
            model: æ¨¡å‹
            epoch: epoch
            metrics: æŒ‡æ ‡
            current_best_metric: å½“å‰æœ€ä½³æŒ‡æ ‡å€¼
            metric_name: ç”¨äºåˆ¤æ–­çš„æŒ‡æ ‡åç§°
            minimize: Trueè¡¨ç¤ºæŒ‡æ ‡è¶Šå°è¶Šå¥½ï¼ŒFalseè¡¨ç¤ºè¶Šå¤§è¶Šå¥½
            save_interval: å¸¸è§„ä¿å­˜é—´éš”
            model_suffix: æ¨¡å‹åç¼€ï¼ˆå¦‚"teacher", "student"ï¼‰
            save_best: æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
        """
        current_metric = metrics.get(metric_name)
        if current_metric is None:
            print(f"Metric '{metric_name}' not found in metrics")
            return self.save_model(model, epoch, metrics, is_best=False, model_suffix=model_suffix), current_best_metric
        
        # check if best
        is_best = False
        new_best_metric = current_best_metric

        if save_best:  # åªæœ‰å½“save_best=Trueæ—¶æ‰åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            if minimize: # switch metric valuation standard by minimize
                if current_metric < current_best_metric:
                    is_best = True
                    new_best_metric = current_metric
            else:
                if current_metric > current_best_metric: # such as acc, the greater the better
                    is_best = True
                    new_best_metric = current_metric

        # save the model
        saved_path = self.save_model(model, epoch, metrics, is_best=is_best, model_suffix=model_suffix)

        if is_best and save_best:
            print(f"BEST: New best model at epoch {epoch} with {metric_name}: {current_metric:.4f}")
        elif epoch % save_interval == 0:
            suffix_info = f" ({model_suffix})" if model_suffix else ""
            print(f"SAVED: Regular checkpoint{suffix_info} saved at epoch {epoch}")

        return saved_path, new_best_metric

    def save_checkpoint_with_hybrid_evaluation(self, model, epoch: int, metrics: Dict,
                                             current_best_loss: float, current_best_miou: float,
                                             loss_threshold: float = 0.02, loss_degradation_threshold: float = 0.01,
                                             save_interval: int = 5, model_suffix: str = "", save_best: bool = True):
        """
        æ··åˆè¯„ä¼°ç­–ç•¥ä¿å­˜checkpointï¼šä¼˜å…ˆä½¿ç”¨lossè¯„ä¼°ï¼Œä¸åˆæ ¼æ—¶ä½¿ç”¨mIoUè¯„ä¼°
        
        Args:
            model: æ¨¡å‹
            epoch: epoch
            metrics: æŒ‡æ ‡å­—å…¸
            current_best_loss: å½“å‰æœ€ä½³losså€¼
            current_best_miou: å½“å‰æœ€ä½³mIoUå€¼
            loss_threshold: lossæ”¹å–„é˜ˆå€¼ï¼Œå°äºæ­¤å€¼æ—¶è®¤ä¸ºlossè¯„ä¼°ä¸åˆæ ¼
            loss_degradation_threshold: lossæ¶åŒ–é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼æ—¶å³ä½¿mIoUæå‡ä¹Ÿä¸ä¿å­˜
            save_interval: å¸¸è§„ä¿å­˜é—´éš”
            model_suffix: æ¨¡å‹åç¼€
            save_best: æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
            
        Returns:
            saved_path, new_best_loss, new_best_miou
        """
        current_loss = metrics.get('val_loss')
        current_miou = metrics.get('miou')
        
        if current_loss is None:
            print(f"Warning: 'val_loss' not found in metrics")
            return self.save_model(model, epoch, metrics, is_best=False, model_suffix=model_suffix), current_best_loss, current_best_miou
            
        if current_miou is None:
            print(f"Warning: 'miou' not found in metrics")
            return self.save_model(model, epoch, metrics, is_best=False, model_suffix=model_suffix), current_best_loss, current_best_miou

        # æ··åˆè¯„ä¼°é€»è¾‘
        is_best = False
        new_best_loss = current_best_loss
        new_best_miou = current_best_miou
        evaluation_reason = ""

        if save_best:
            # ç­–ç•¥1: ä¼˜å…ˆä½¿ç”¨lossè¯„ä¼°
            loss_improvement = current_best_loss - current_loss
            if loss_improvement > loss_threshold:
                # lossæ”¹å–„è¶³å¤Ÿå¤§ï¼Œä½¿ç”¨lossè¯„ä¼°
                is_best = True
                new_best_loss = current_loss
                new_best_miou = current_miou  # åŒæ—¶æ›´æ–°mIoUè®°å½•
                evaluation_reason = f"Loss improved by {loss_improvement:.4f} (>{loss_threshold:.4f})"
            else:
                # ç­–ç•¥2: lossæ”¹å–„ä¸è¶³ï¼Œä½¿ç”¨mIoUè¯„ä¼°
                # ä½†å¦‚æœlossæ˜¾è‘—å¢åŠ ï¼Œå³ä½¿mIoUæå‡ä¹Ÿè¦è°¨æ…
                if current_miou > current_best_miou:
                    if loss_improvement >= -loss_degradation_threshold:
                        # lossæ²¡æœ‰æ˜¾è‘—æ¶åŒ–ï¼Œå¯ä»¥åŸºäºmIoUä¿å­˜
                        is_best = True
                        new_best_miou = current_miou
                        # losså¯èƒ½æœ‰å°å¹…æ”¹å–„ï¼Œä¹Ÿæ›´æ–°è®°å½•
                        if current_loss < current_best_loss:
                            new_best_loss = current_loss
                        evaluation_reason = f"Loss improvement insufficient ({loss_improvement:.4f}<={loss_threshold:.4f}), but mIoU improved: {current_miou:.4f} > {current_best_miou:.4f}"
                    else:
                        # lossæ˜¾è‘—æ¶åŒ–ï¼Œå³ä½¿mIoUæå‡ä¹Ÿä¸ä¿å­˜
                        evaluation_reason = f"Loss degraded significantly ({loss_improvement:.4f}<-{loss_degradation_threshold:.4f}), ignoring mIoU improvement: {current_miou:.4f} > {current_best_miou:.4f}"
                else:
                    evaluation_reason = f"Neither loss nor mIoU improved sufficiently (loss: {loss_improvement:.4f}, mIoU: {current_miou:.4f} <= {current_best_miou:.4f})"

        # ä¿å­˜æ¨¡å‹
        saved_path = self.save_model(model, epoch, metrics, is_best=is_best, model_suffix=model_suffix)

        if is_best and save_best:
            print(f"ğŸ¯ BEST: New best model at epoch {epoch}")
            print(f"   ğŸ“Š Evaluation: {evaluation_reason}")
            print(f"   ğŸ“ˆ Metrics: loss={current_loss:.4f}, mIoU={current_miou:.4f}")
        elif epoch % save_interval == 0:
            suffix_info = f" ({model_suffix})" if model_suffix else ""
            print(f"SAVED: Regular checkpoint{suffix_info} saved at epoch {epoch}")
            print(f"   ğŸ“Š {evaluation_reason}")

        return saved_path, new_best_loss, new_best_miou


    def _update_best_model_record(self, epoch: int, metrics: Dict, model_path: str):
        best_record_path = os.path.join(self.run_dir, "best_model_info.json")
        best_info = {
            'best_epoch': epoch,
            'best_metrics': metrics,
            'model_path': model_path,
            'timestamp': self.timestamp,
            'updated_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        with open(best_record_path, 'w') as f:
            json.dump(best_info, f, indent=2)  

    def get_best_model_path(self, model_suffix: str = "") -> Optional[str]:
        checkpoints_dir = self.get_checkpoints_dir()
        model_name = f"{self.model_type}_{model_suffix}" if model_suffix else self.model_type
        candidate = os.path.join(checkpoints_dir, f"{model_name}_best.pth")
        if os.path.exists(candidate):
            return candidate

        record_path = os.path.join(self.run_dir, "best_model_info.json")
        if os.path.exists(record_path):
            try:
                with open(record_path, "r", encoding="utf-8") as f:
                    info = json.load(f)
                stored_path = info.get("model_path")
                if stored_path and os.path.exists(stored_path):
                    return stored_path
            except (OSError, json.JSONDecodeError):
                return None

        return None


    # get latest checkpoint
    def _get_latest_checkpoint(self) -> str:
        # get path
        checkpoints_dir = self.get_checkpoints_dir()

        # locate all ck files
        if os.path.exists(checkpoints_dir):
            checkpoints = [f for f in os.listdir(checkpoints_dir) if f.endswith(".pth")]
            # retrive latest model
            latest = max(checkpoints, key = lambda x: os.path.getctime(os.path.join(checkpoints_dir, x)))
            return os.path.join(checkpoints_dir, latest)

        return ""

    # gain metrics history
    def get_metrics_history(self) -> list:
        # get csv path
        csv_path = self.get_metrics_csv_path()
        if not os.path.exists(csv_path):
            return []
        
        try:
            import pandas as pd
            df = pd.read_csv(csv_path)
            return df.to_dict('records')  # convert to list of dicts
        except Exception as e:
            print(f"Error reading metrics CSV: {e}")
            return []
        
    def get_kd_experiment_dir(self) -> str:
        """è·å–KDå®éªŒä¸“ç”¨ç›®å½•"""
        kd_dir = os.path.join(self.run_dir, "kd_experiment")
        os.makedirs(kd_dir, exist_ok=True)
        return kd_dir
    
    def get_kd_comparison_csv_path(self) -> str:
        """è·å–KDå¯¹æ¯”è¡¨æ ¼CSVè·¯å¾„"""
        return os.path.join(self.get_kd_experiment_dir(), "kd_comparison_results.csv")
    
    def get_calibration_analysis_dir(self) -> str:
        """è·å–æ ¡å‡†åˆ†æç›®å½•"""
        cal_dir = os.path.join(self.run_dir, "calibration_analysis")
        os.makedirs(cal_dir, exist_ok=True)
        return cal_dir
    
    def get_reliability_diagram_path(self, regime_name: str) -> str:
        """è·å–å¯é æ€§å›¾ä¿å­˜è·¯å¾„"""
        return os.path.join(self.get_calibration_analysis_dir(), 
                           f"reliability_diagram_{regime_name}.png")
    
    def save_kd_experiment_summary(self, experiment_data: Dict):
        """ä¿å­˜KDå®éªŒæ€»ç»“"""
        summary_path = os.path.join(self.get_kd_experiment_dir(), "experiment_summary.json")
        
        summary = {
            "timestamp": self.timestamp,
            "experiment_type": "knowledge_distillation_comparison",
            "run_dir": self.run_dir,
            "experiment_data": experiment_data,
            "files": {
                "comparison_csv": self.get_kd_comparison_csv_path(),
                "calibration_dir": self.get_calibration_analysis_dir(),
                "visualizations": self.get_vis_dir()
            }
        }
        
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"KD experiment summary saved to: {summary_path}")
        return summary_path